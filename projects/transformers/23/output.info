"commit_hash": "9aeacb58bab321bc21c24bbdf7a24efdccb1d426",
"commit_link": "https://github.com/huggingface/transformers/commit/9aeacb58bab321bc21c24bbdf7a24efdccb1d426",
"files": ["docs/source/model_doc/transformerxl.rst", "setup.py", "src/transformers/__init__.py", "src/transformers/convert_slow_tokenizer.py", "src/transformers/tokenization_albert.py", "src/transformers/tokenization_auto.py", "src/transformers/tokenization_bart.py", "src/transformers/tokenization_bert.py", "src/transformers/tokenization_bert_japanese.py", "src/transformers/tokenization_bertweet.py", "src/transformers/tokenization_camembert.py", "src/transformers/tokenization_distilbert.py", "src/transformers/tokenization_dpr.py", "src/transformers/tokenization_electra.py", "src/transformers/tokenization_fsmt.py", "src/transformers/tokenization_funnel.py", "src/transformers/tokenization_gpt2.py", "src/transformers/tokenization_longformer.py", "src/transformers/tokenization_lxmert.py", "src/transformers/tokenization_mbart.py", "src/transformers/tokenization_mobilebert.py", "src/transformers/tokenization_openai.py", "src/transformers/tokenization_pegasus.py", "src/transformers/tokenization_phobert.py", "src/transformers/tokenization_reformer.py", "src/transformers/tokenization_retribert.py", "src/transformers/tokenization_roberta.py", "src/transformers/tokenization_t5.py", "src/transformers/tokenization_transfo_xl.py", "src/transformers/tokenization_utils.py", "src/transformers/tokenization_utils_base.py", "src/transformers/tokenization_utils_fast.py", "src/transformers/tokenization_xlm.py", "src/transformers/tokenization_xlm_roberta.py", "src/transformers/tokenization_xlnet.py", "src/transformers/utils/sentencepiece_model_pb2.py", "tests/test_tokenization_albert.py", "tests/test_tokenization_bart.py", "tests/test_tokenization_bert.py", "tests/test_tokenization_bert_japanese.py", "tests/test_tokenization_camembert.py", "tests/test_tokenization_common.py", "tests/test_tokenization_ctrl.py", "tests/test_tokenization_distilbert.py", "tests/test_tokenization_dpr.py", "tests/test_tokenization_fast.py", "tests/test_tokenization_funnel.py", "tests/test_tokenization_gpt2.py", "tests/test_tokenization_lxmert.py", "tests/test_tokenization_marian.py", "tests/test_tokenization_mbart.py", "tests/test_tokenization_openai.py", "tests/test_tokenization_pegasus.py", "tests/test_tokenization_reformer.py", "tests/test_tokenization_roberta.py", "tests/test_tokenization_t5.py", "tests/test_tokenization_transfo_xl.py", "tests/test_tokenization_xlm.py", "tests/test_tokenization_xlm_roberta.py", "tests/test_tokenization_xlnet.py", ]
"type": "assert"
"numberOfMigration": ""
